{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04a0ad08-e260-433f-98e5-3fb79cd239b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset object from JSON file: cuad/CUAD_v1/CUAD_v1.json\n",
      "Dataset object loaded successfully!\n",
      "\n",
      "Columns in the final, correct DataFrame:\n",
      "Index(['title', 'paragraphs'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "json_file_path = \"cuad/CUAD_v1/CUAD_v1.json\"\n",
    "\n",
    "print(f\"Loading dataset object from JSON file: {json_file_path}\")\n",
    "cuad_dataset = load_dataset('json', data_files=json_file_path, split=\"train\")\n",
    "print(\"Dataset object loaded successfully!\")\n",
    "\n",
    "\n",
    "# --- THE CORRECTED LINE ---\n",
    "# The dataset has one row. We access it with [0], then get the list of contracts from the 'data' key.\n",
    "df = pd.DataFrame(cuad_dataset[0]['data'])\n",
    "\n",
    "\n",
    "# --- VERIFICATION STEP ---\n",
    "print(\"\\nColumns in the final, correct DataFrame:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfcdddbd-73fc-4404-816b-45e1d9d69829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bytes': None, 'path': 'C:\\Users\\barat\\Projec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bytes': None, 'path': 'C:\\Users\\barat\\Projec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bytes': None, 'path': 'C:\\Users\\barat\\Projec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bytes': None, 'path': 'C:\\Users\\barat\\Projec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bytes': None, 'path': 'C:\\Users\\barat\\Projec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 pdf\n",
       "0  {'bytes': None, 'path': 'C:\\Users\\barat\\Projec...\n",
       "1  {'bytes': None, 'path': 'C:\\Users\\barat\\Projec...\n",
       "2  {'bytes': None, 'path': 'C:\\Users\\barat\\Projec...\n",
       "3  {'bytes': None, 'path': 'C:\\Users\\barat\\Projec...\n",
       "4  {'bytes': None, 'path': 'C:\\Users\\barat\\Projec..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9884ee3e-5fe4-437b-9994-f5b8768ecd38",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'question'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get a list of all unique clause types from your DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m unique_clauses = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.unique().tolist()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Define the full list of labels in IOB2 format (B- for beginning, I- for inside)\u001b[39;00m\n\u001b[32m      5\u001b[39m labels_list = [\u001b[33m\"\u001b[39m\u001b[33mO\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;66;03m# 'O' is for tokens Outside any clause\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'question'"
     ]
    }
   ],
   "source": [
    "# Get a list of all unique clause types from your DataFrame\n",
    "unique_clauses = df['question'].unique().tolist()\n",
    "\n",
    "# Define the full list of labels in IOB2 format (B- for beginning, I- for inside)\n",
    "labels_list = [\"O\"] # 'O' is for tokens Outside any clause\n",
    "for clause in unique_clauses:\n",
    "    label_name = clause.replace(\"What is the \", \"\").replace(\"?\", \"\").replace(\" \", \"_\").upper()\n",
    "    labels_list.append(f\"B-{label_name}\")\n",
    "    labels_list.append(f\"I-{label_name}\")\n",
    "\n",
    "# Create the dictionaries that map labels to IDs and vice-versa\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for i, label in enumerate(labels_list)}\n",
    "\n",
    "print(f\"Total Labels Created: {len(labels_list)}\")\n",
    "print(\"Example label mapping:\", list(label2id.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8620a4d3-9c77-4c5d-93cd-ed6f9973f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=510, step=1)\n",
      "RangeIndex(start=0, stop=510, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)# Confirm the columns are what we expect\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34754c69-a77f-4bc7-b031-ecbdd12c3836",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get a list of all unique clause types from your DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m unique_clauses = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.unique().tolist()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Define the full list of labels in IOB2 format (B- for beginning, I- for inside)\u001b[39;00m\n\u001b[32m      5\u001b[39m labels_list = [\u001b[33m\"\u001b[39m\u001b[33mO\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;66;03m# 'O' is for tokens Outside any clause\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'question'"
     ]
    }
   ],
   "source": [
    "# Get a list of all unique clause types from your DataFrame\n",
    "unique_clauses = df['question'].unique().tolist()\n",
    "\n",
    "# Define the full list of labels in IOB2 format (B- for beginning, I- for inside)\n",
    "labels_list = [\"O\"] # 'O' is for tokens Outside any clause\n",
    "for clause in unique_clauses:\n",
    "    label_name = clause.replace(\"What is the \", \"\").replace(\"?\", \"\").replace(\" \", \"_\").upper()\n",
    "    labels_list.append(f\"B-{label_name}\")\n",
    "    labels_list.append(f\"I-{label_name}\")\n",
    "\n",
    "# Create the dictionaries that map labels to IDs and vice-versa\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for i, label in enumerate(labels_list)}\n",
    "\n",
    "print(f\"Total Labels Created: {len(labels_list)}\")\n",
    "print(\"Example label mapping:\", list(label2id.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cd70aa8-084c-443a-82f8-0bdadb646ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking the nested data... this may take a moment.\n",
      "\n",
      "Flattening complete!\n",
      "Columns in the final, correct DataFrame:\n",
      "Index(['id', 'title', 'context', 'question', 'answers'], dtype='object')\n",
      "\n",
      "First 5 rows of the final DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>EXHIBIT 10.6\\n\\n                              ...</td>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "      <td>[{'text': 'DISTRIBUTOR AGREEMENT', 'answer_sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>EXHIBIT 10.6\\n\\n                              ...</td>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "      <td>[{'text': 'Distributor', 'answer_start': 244},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>EXHIBIT 10.6\\n\\n                              ...</td>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "      <td>[{'text': '7th day of September, 1999.', 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>EXHIBIT 10.6\\n\\n                              ...</td>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "      <td>[{'text': 'The term of this  Agreement  shall ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>EXHIBIT 10.6\\n\\n                              ...</td>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "      <td>[{'text': 'The term of this  Agreement  shall ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "1  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "2  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "3  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "4  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "\n",
       "                                               title  \\\n",
       "0  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "1  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "2  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "3  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "4  LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "\n",
       "                                             context  \\\n",
       "0  EXHIBIT 10.6\\n\\n                              ...   \n",
       "1  EXHIBIT 10.6\\n\\n                              ...   \n",
       "2  EXHIBIT 10.6\\n\\n                              ...   \n",
       "3  EXHIBIT 10.6\\n\\n                              ...   \n",
       "4  EXHIBIT 10.6\\n\\n                              ...   \n",
       "\n",
       "                                            question  \\\n",
       "0  Highlight the parts (if any) of this contract ...   \n",
       "1  Highlight the parts (if any) of this contract ...   \n",
       "2  Highlight the parts (if any) of this contract ...   \n",
       "3  Highlight the parts (if any) of this contract ...   \n",
       "4  Highlight the parts (if any) of this contract ...   \n",
       "\n",
       "                                             answers  \n",
       "0  [{'text': 'DISTRIBUTOR AGREEMENT', 'answer_sta...  \n",
       "1  [{'text': 'Distributor', 'answer_start': 244},...  \n",
       "2  [{'text': '7th day of September, 1999.', 'answ...  \n",
       "3  [{'text': 'The term of this  Agreement  shall ...  \n",
       "4  [{'text': 'The term of this  Agreement  shall ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This assumes 'df' is your current DataFrame with ['title', 'paragraphs'] columns\n",
    "\n",
    "flattened_data = []\n",
    "print(\"Unpacking the nested data... this may take a moment.\")\n",
    "\n",
    "# Iterate through each contract (each row in the current df)\n",
    "for index, row in df.iterrows():\n",
    "    contract_title = row['title']\n",
    "    \n",
    "    # Each contract has a list of paragraphs\n",
    "    for para in row['paragraphs']:\n",
    "        context = para['context']\n",
    "        \n",
    "        # Each paragraph has a list of question-answer sets (qas)\n",
    "        for qa in para['qas']:\n",
    "            # For each question, create a single flat record\n",
    "            record = {\n",
    "                'id': qa['id'],\n",
    "                'title': contract_title,\n",
    "                'context': context,\n",
    "                'question': qa['question'],\n",
    "                'answers': qa['answers']\n",
    "            }\n",
    "            flattened_data.append(record)\n",
    "\n",
    "# Create the final, correct DataFrame from our new flattened list\n",
    "final_df = pd.DataFrame(flattened_data)\n",
    "\n",
    "\n",
    "# --- VERIFICATION STEP ---\n",
    "print(\"\\nFlattening complete!\")\n",
    "print(\"Columns in the final, correct DataFrame:\")\n",
    "print(final_df.columns)\n",
    "\n",
    "print(\"\\nFirst 5 rows of the final DataFrame:\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fae33023-c1ad-4094-8776-cbcf57110372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Labels Created: 83\n",
      "Example label mapping: [('O', 0), ('B-HIGHLIGHT_THE_PARTS_(IF_ANY)_OF_THIS_CONTRACT_RELATED_TO_\"DOCUMENT_NAME\"_THAT_SHOULD_BE_REVIEWED_BY_A_LAWYER._DETAILS:_THE_NAME_OF_THE_CONTRACT', 1), ('I-HIGHLIGHT_THE_PARTS_(IF_ANY)_OF_THIS_CONTRACT_RELATED_TO_\"DOCUMENT_NAME\"_THAT_SHOULD_BE_REVIEWED_BY_A_LAWYER._DETAILS:_THE_NAME_OF_THE_CONTRACT', 2), ('B-HIGHLIGHT_THE_PARTS_(IF_ANY)_OF_THIS_CONTRACT_RELATED_TO_\"PARTIES\"_THAT_SHOULD_BE_REVIEWED_BY_A_LAWYER._DETAILS:_THE_TWO_OR_MORE_PARTIES_WHO_SIGNED_THE_CONTRACT', 3), ('I-HIGHLIGHT_THE_PARTS_(IF_ANY)_OF_THIS_CONTRACT_RELATED_TO_\"PARTIES\"_THAT_SHOULD_BE_REVIEWED_BY_A_LAWYER._DETAILS:_THE_TWO_OR_MORE_PARTIES_WHO_SIGNED_THE_CONTRACT', 4)]\n"
     ]
    }
   ],
   "source": [
    "# Use your final, correct DataFrame 'final_df'\n",
    "unique_clauses = final_df['question'].unique().tolist()\n",
    "\n",
    "# Define the full list of labels in IOB2 format (B- for beginning, I- for inside)\n",
    "labels_list = [\"O\"] # 'O' is for tokens Outside any clause\n",
    "for clause in unique_clauses:\n",
    "    label_name = clause.replace(\"What is the \", \"\").replace(\"?\", \"\").replace(\" \", \"_\").upper()\n",
    "    labels_list.append(f\"B-{label_name}\")\n",
    "    labels_list.append(f\"I-{label_name}\")\n",
    "\n",
    "# Create the dictionaries that map labels to IDs and vice-versa\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for i, label in enumerate(labels_list)}\n",
    "\n",
    "print(f\"Total Labels Created: {len(labels_list)}\")\n",
    "print(\"Example label mapping:\", list(label2id.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a1ffa-421a-487f-8780-20d4a90f4cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aaaba8c-ccb1-4624-bf7d-7b27c4732ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f07bbe6f-c73c-4bfb-bdf8-53f1b5384657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_ner_full(examples):\n",
    "    # This function processes a batch of examples from the dataset.\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"context\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(examples[\"question\"])):\n",
    "        question = examples[\"question\"][i]\n",
    "        answers = examples[\"answers\"][i]\n",
    "\n",
    "        label_name = question.replace(\"What is the \", \"\").replace(\"?\", \"\").replace(\" \", \"_\").upper()\n",
    "        b_label_id = label2id[f\"B-{label_name}\"]\n",
    "        i_label_id = label2id[f\"I-{label_name}\"]\n",
    "\n",
    "        # --- FIX IS HERE ---\n",
    "        # Check if 'answers' is a dictionary and has a non-empty 'answer_start' list\n",
    "        if isinstance(answers, dict) and answers.get(\"answer_start\"):\n",
    "            answer_start = answers[\"answer_start\"][0]\n",
    "            answer_end = answer_start + len(answers[\"text\"][0])\n",
    "        else:\n",
    "            # If answers is empty or not a dict, there is no answer span\n",
    "            answer_start = -1\n",
    "            answer_end = -1\n",
    "        # --- END OF FIX ---\n",
    "\n",
    "        doc_chunk_indices = [j for j, sample_idx in enumerate(sample_mapping) if sample_idx == i]\n",
    "\n",
    "        for chunk_idx in doc_chunk_indices:\n",
    "            chunk_labels = [label2id[\"O\"]] * len(tokenized_inputs[\"input_ids\"][chunk_idx])\n",
    "            chunk_offset_mapping = offset_mapping[chunk_idx]\n",
    "\n",
    "            token_start_index = 0\n",
    "            while token_start_index < len(chunk_offset_mapping) and chunk_offset_mapping[token_start_index][0] < answer_start:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = token_start_index\n",
    "            while token_end_index < len(chunk_offset_mapping) and chunk_offset_mapping[token_end_index][1] <= answer_end:\n",
    "                token_end_index += 1\n",
    "            token_end_index -= 1\n",
    "\n",
    "            if answer_start != -1 and token_start_index <= token_end_index:\n",
    "                chunk_labels[token_start_index] = b_label_id\n",
    "                for j in range(token_start_index + 1, token_end_index + 1):\n",
    "                    chunk_labels[j] = i_label_id\n",
    "\n",
    "            labels.append(chunk_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "465cb7c2-33b0-42e1-b5a8-0d59e5679902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f389bdb274ae4591aaeaf2d783b3616b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "The processed dataset is ready for training:\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 596591\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert your final pandas DataFrame into a Hugging Face Dataset object\n",
    "full_dataset = Dataset.from_pandas(final_df)\n",
    "\n",
    "# Apply the powerful .map() function to process the entire dataset\n",
    "processed_dataset = full_dataset.map(\n",
    "    preprocess_for_ner_full,\n",
    "    batched=True, \n",
    "    remove_columns=full_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "print(\"The processed dataset is ready for training:\")\n",
    "print(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321aad82-beae-4baa-a255-61b7d22a7b9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForTokenClassification, TrainingArguments, Trainer\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load the pre-trained model. We provide the number of labels and the label mappings.\u001b[39;00m\n\u001b[32m      4\u001b[39m model = AutoModelForTokenClassification.from_pretrained(\n\u001b[32m      5\u001b[39m     model_checkpoint, \n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     num_labels=\u001b[38;5;28mlen\u001b[39m(\u001b[43mlabels_list\u001b[49m),\n\u001b[32m      7\u001b[39m     id2label=id2label,\n\u001b[32m      8\u001b[39m     label2id=label2id\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Define the training arguments\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 'output_dir' is where the trained model will be saved\u001b[39;00m\n\u001b[32m     13\u001b[39m training_args = TrainingArguments(\n\u001b[32m     14\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33mlegal-ner-model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     learning_rate=\u001b[32m2e-5\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     load_best_model_at_end=\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Optional: Load the best performing model at the end\u001b[39;00m\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'labels_list' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load the pre-trained model. We provide the number of labels and the label mappings.\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=len(labels_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "# 'output_dir' is where the trained model will be saved\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"legal-ner-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", # Optional: Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",       # Optional: Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True, # Optional: Load the best performing model at the end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773ffbca-0ca8-40b6-b916-3ad457c658e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Complete: final_df is created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- STEP 1: Load the data and create the final_df ---\n",
    "# This block re-does the entire flattening process.\n",
    "json_file_path = \"cuad/CUAD_v1/CUAD_v1.json\"\n",
    "cuad_dataset = load_dataset('json', data_files=json_file_path, split=\"train\")\n",
    "df_nested = pd.DataFrame(cuad_dataset[0]['data'])\n",
    "\n",
    "flattened_data = []\n",
    "for index, row in df_nested.iterrows():\n",
    "    contract_title = row['title']\n",
    "    for para in row['paragraphs']:\n",
    "        context = para['context']\n",
    "        for qa in para['qas']:\n",
    "            record = {\n",
    "                'id': qa['id'],\n",
    "                'title': contract_title,\n",
    "                'context': context,\n",
    "                'question': qa['question'],\n",
    "                'answers': qa['answers']\n",
    "            }\n",
    "            flattened_data.append(record)\n",
    "final_df = pd.DataFrame(flattened_data)\n",
    "print(\"Step 1 Complete: final_df is created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b3b962-f679-4d50-afbb-e2207f25035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 Complete: Label mappings are created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 Complete: Tokenizer is initialized.\n",
      "Step 4 Complete: Preprocessing function is defined.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 2: Create Label Mappings ---\n",
    "unique_clauses = final_df['question'].unique().tolist()\n",
    "labels_list = [\"O\"]\n",
    "for clause in unique_clauses:\n",
    "    label_name = clause.replace(\"What is the \", \"\").replace(\"?\", \"\").replace(\" \", \"_\").upper()\n",
    "    labels_list.append(f\"B-{label_name}\")\n",
    "    labels_list.append(f\"I-{label_name}\")\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for i, label in enumerate(labels_list)}\n",
    "print(\"Step 2 Complete: Label mappings are created.\")\n",
    "\n",
    "# --- STEP 3: Initialize Tokenizer ---\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(\"Step 3 Complete: Tokenizer is initialized.\")\n",
    "\n",
    "# --- STEP 4: Define the Preprocessing Function ---\n",
    "def preprocess_for_ner_full(examples):\n",
    "    # (The same long function you had before)\n",
    "    tokenized_inputs = tokenizer(examples[\"context\"], max_length=512, truncation=True, stride=128, return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\")\n",
    "    sample_mapping = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_inputs.pop(\"offset_mapping\")\n",
    "    labels = []\n",
    "    for i in range(len(examples[\"question\"])):\n",
    "        question = examples[\"question\"][i]\n",
    "        answers = examples[\"answers\"][i]\n",
    "        label_name = question.replace(\"What is the \", \"\").replace(\"?\", \"\").replace(\" \", \"_\").upper()\n",
    "        b_label_id = label2id[f\"B-{label_name}\"]\n",
    "        i_label_id = label2id[f\"I-{label_name}\"]\n",
    "        if isinstance(answers, dict) and answers.get(\"answer_start\"):\n",
    "            answer_start = answers[\"answer_start\"][0]\n",
    "            answer_end = answer_start + len(answers[\"text\"][0])\n",
    "        else:\n",
    "            answer_start, answer_end = -1, -1\n",
    "        doc_chunk_indices = [j for j, sample_idx in enumerate(sample_mapping) if sample_idx == i]\n",
    "        for chunk_idx in doc_chunk_indices:\n",
    "            chunk_labels = [label2id[\"O\"]] * len(tokenized_inputs[\"input_ids\"][chunk_idx])\n",
    "            chunk_offset_mapping = offset_mapping[chunk_idx]\n",
    "            token_start_index = 0\n",
    "            while token_start_index < len(chunk_offset_mapping) and chunk_offset_mapping[token_start_index][0] < answer_start:\n",
    "                token_start_index += 1\n",
    "            token_end_index = token_start_index\n",
    "            while token_end_index < len(chunk_offset_mapping) and chunk_offset_mapping[token_end_index][1] <= answer_end:\n",
    "                token_end_index += 1\n",
    "            token_end_index -= 1\n",
    "            if answer_start != -1 and token_start_index <= token_end_index:\n",
    "                chunk_labels[token_start_index] = b_label_id\n",
    "                for j in range(token_start_index + 1, token_end_index + 1):\n",
    "                    chunk_labels[j] = i_label_id\n",
    "            labels.append(chunk_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "print(\"Step 4 Complete: Preprocessing function is defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aaa8386-a017-45a3-9ef2-4136421b4295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 Complete: Label mappings are created.\n",
      "Step 3 Complete: Tokenizer is initialized.\n",
      "Step 4 Complete: Preprocessing function is defined.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 2: Create Label Mappings ---\n",
    "unique_clauses = final_df['question'].unique().tolist()\n",
    "labels_list = [\"O\"]\n",
    "for clause in unique_clauses:\n",
    "    label_name = clause.replace(\"What is the \", \"\").replace(\"?\", \"\").replace(\" \", \"_\").upper()\n",
    "    labels_list.append(f\"B-{label_name}\")\n",
    "    labels_list.append(f\"I-{label_name}\")\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for i, label in enumerate(labels_list)}\n",
    "print(\"Step 2 Complete: Label mappings are created.\")\n",
    "\n",
    "# --- STEP 3: Initialize Tokenizer ---\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(\"Step 3 Complete: Tokenizer is initialized.\")\n",
    "\n",
    "# --- STEP 4: Define the Preprocessing Function ---\n",
    "def preprocess_for_ner_full(examples):\n",
    "    # (The same long function you had before)\n",
    "    tokenized_inputs = tokenizer(examples[\"context\"], max_length=512, truncation=True, stride=128, return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\")\n",
    "    sample_mapping = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_inputs.pop(\"offset_mapping\")\n",
    "    labels = []\n",
    "    for i in range(len(examples[\"question\"])):\n",
    "        question = examples[\"question\"][i]\n",
    "        answers = examples[\"answers\"][i]\n",
    "        label_name = question.replace(\"What is the \", \"\").replace(\"?\", \"\").replace(\" \", \"_\").upper()\n",
    "        b_label_id = label2id[f\"B-{label_name}\"]\n",
    "        i_label_id = label2id[f\"I-{label_name}\"]\n",
    "        if isinstance(answers, dict) and answers.get(\"answer_start\"):\n",
    "            answer_start = answers[\"answer_start\"][0]\n",
    "            answer_end = answer_start + len(answers[\"text\"][0])\n",
    "        else:\n",
    "            answer_start, answer_end = -1, -1\n",
    "        doc_chunk_indices = [j for j, sample_idx in enumerate(sample_mapping) if sample_idx == i]\n",
    "        for chunk_idx in doc_chunk_indices:\n",
    "            chunk_labels = [label2id[\"O\"]] * len(tokenized_inputs[\"input_ids\"][chunk_idx])\n",
    "            chunk_offset_mapping = offset_mapping[chunk_idx]\n",
    "            token_start_index = 0\n",
    "            while token_start_index < len(chunk_offset_mapping) and chunk_offset_mapping[token_start_index][0] < answer_start:\n",
    "                token_start_index += 1\n",
    "            token_end_index = token_start_index\n",
    "            while token_end_index < len(chunk_offset_mapping) and chunk_offset_mapping[token_end_index][1] <= answer_end:\n",
    "                token_end_index += 1\n",
    "            token_end_index -= 1\n",
    "            if answer_start != -1 and token_start_index <= token_end_index:\n",
    "                chunk_labels[token_start_index] = b_label_id\n",
    "                for j in range(token_start_index + 1, token_end_index + 1):\n",
    "                    chunk_labels[j] = i_label_id\n",
    "            labels.append(chunk_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "print(\"Step 4 Complete: Preprocessing function is defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67fdec0a-517c-4f88-af4c-c06aac93dee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04366f472794f168e9f8b32a5dd2dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 Complete: Processed dataset is ready!\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 596591\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 5: Recreate the Processed Dataset ---\n",
    "full_dataset = Dataset.from_pandas(final_df)\n",
    "processed_dataset = full_dataset.map(\n",
    "    preprocess_for_ner_full,\n",
    "    batched=True, \n",
    "    remove_columns=full_dataset.column_names\n",
    ")\n",
    "print(\"Step 5 Complete: Processed dataset is ready!\")\n",
    "print(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376f4849-de38-4b9f-a8f2-2b3143fdbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! CUDA is available. Your NVIDIA GPU will be used for training.\n",
      "GPU Device Name: NVIDIA GeForce RTX 5070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ Success! CUDA is available. Your NVIDIA GPU will be used for training.\")\n",
    "    print(f\"GPU Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"❌ CUDA not available. Training will run on the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecddba5f-d211-4358-9c85-2cee7c8f9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FINAL ALTERNATIVE: LEGACY TRAINING ARGUMENTS ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"legal-ner-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # -- Legacy Arguments --\n",
    "    # This line is important to see validation results during training\n",
    "    evaluate_during_training=True,  # <--- ADD THIS LINE BACK\n",
    "    \n",
    "    logging_steps=500,\n",
    "    save_steps=500,\n",
    "    \n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Re-create the trainer with the corrected arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    eval_dataset=processed_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b130d-89b0-41b3-ad05-bcf73c54b921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5d268e-0c18-46bd-9b4d-f8132b98f624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1: Data loaded and flattened successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc11ce8f10b4fbdb1b4a266ef2a771b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 2: Preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 3: Trainer is set up and ready to train.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:645: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# STEP 1: LIBRARIES AND DATA LOADING\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load and flatten the data from your local files\n",
    "json_file_path = \"cuad/CUAD_v1/CUAD_v1.json\"\n",
    "cuad_dataset = load_dataset('json', data_files=json_file_path, split=\"train\")\n",
    "df_nested = pd.DataFrame(cuad_dataset[0]['data'])\n",
    "\n",
    "flattened_data = []\n",
    "for index, row in df_nested.iterrows():\n",
    "    contract_title = row['title']\n",
    "    for para in row['paragraphs']:\n",
    "        context = para['context']\n",
    "        for qa in para['qas']:\n",
    "            record = {'id': qa['id'], 'title': contract_title, 'context': context, 'question': qa['question'], 'answers': qa['answers']}\n",
    "            flattened_data.append(record)\n",
    "final_df = pd.DataFrame(flattened_data)\n",
    "print(\"✅ Step 1: Data loaded and flattened successfully.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: MAPPINGS, TOKENIZER, AND PREPROCESSING FUNCTION\n",
    "# ==============================================================================\n",
    "unique_clauses = final_df['question'].unique().tolist()\n",
    "labels_list = [\"O\"]\n",
    "for clause in unique_clauses:\n",
    "    label_name = clause.replace(\"What is the \", \"\").replace(\"?\", \"\").replace(\" \", \"_\").upper()\n",
    "    labels_list.append(f\"B-{label_name}\")\n",
    "    labels_list.append(f\"I-{label_name}\")\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for i, label in enumerate(labels_list)}\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_for_ner_full(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"context\"], max_length=512, truncation=True, stride=128, return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\")\n",
    "    sample_mapping = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_inputs.pop(\"offset_mapping\")\n",
    "    labels = []\n",
    "    for i in range(len(examples[\"question\"])):\n",
    "        question = examples[\"question\"][i]; answers = examples[\"answers\"][i]\n",
    "        label_name = question.replace(\"What is the \", \"\").replace(\"?\", \"\").replace(\" \", \"_\").upper()\n",
    "        b_label_id = label2id[f\"B-{label_name}\"]; i_label_id = label2id[f\"I-{label_name}\"]\n",
    "        if isinstance(answers, dict) and answers.get(\"answer_start\"):\n",
    "            answer_start = answers[\"answer_start\"][0]; answer_end = answer_start + len(answers[\"text\"][0])\n",
    "        else:\n",
    "            answer_start, answer_end = -1, -1\n",
    "        doc_chunk_indices = [j for j, sample_idx in enumerate(sample_mapping) if sample_idx == i]\n",
    "        for chunk_idx in doc_chunk_indices:\n",
    "            chunk_labels = [label2id[\"O\"]] * len(tokenized_inputs[\"input_ids\"][chunk_idx]); chunk_offset_mapping = offset_mapping[chunk_idx]\n",
    "            token_start_index = 0\n",
    "            while token_start_index < len(chunk_offset_mapping) and chunk_offset_mapping[token_start_index][0] < answer_start: token_start_index += 1\n",
    "            token_end_index = token_start_index\n",
    "            while token_end_index < len(chunk_offset_mapping) and chunk_offset_mapping[token_end_index][1] <= answer_end: token_end_index += 1\n",
    "            token_end_index -= 1\n",
    "            if answer_start != -1 and token_start_index <= token_end_index:\n",
    "                chunk_labels[token_start_index] = b_label_id\n",
    "                for j in range(token_start_index + 1, token_end_index + 1): chunk_labels[j] = i_label_id\n",
    "            labels.append(chunk_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "full_dataset = Dataset.from_pandas(final_df)\n",
    "processed_dataset = full_dataset.map(preprocess_for_ner_full, batched=True, remove_columns=full_dataset.column_names)\n",
    "print(\"✅ Step 2: Preprocessing complete.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 3: SETUP TRAINER WITH LEGACY & OPTIMIZED ARGUMENTS\n",
    "# ==============================================================================\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(labels_list), id2label=id2label, label2id=label2id)\n",
    "# ✅ This is the modern, recommended way to set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"legal-ner-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,              # <-- You must add this line\n",
    "    \n",
    "    load_best_model_at_end=True, # Recommended for getting the best model\n",
    "    save_total_limit=2,          # Good for saving disk space\n",
    "    fp16=True                    # For GPU speedup\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    eval_dataset=processed_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"✅ Step 3: Trainer is set up and ready to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0217f73f-1898-4751-98ea-097a0d254d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a small subset for quick training:\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 15000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "quick_train_subset = processed_dataset.select(range(15000))\n",
    "\n",
    "print(\"Created a small subset for quick training:\")\n",
    "print(quick_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d66675-b3f5-4409-87bd-f9781a8aa7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:645: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Re-create the Trainer using the SMALL subset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=quick_train_subset,\n",
    "    eval_dataset=quick_train_subset, # Use the same small set for evaluation\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c213e9-e5b1-4d8e-992b-7ee0d16a6df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 27:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "C:\\Users\\barat\\Projects\\Legal_AI_Project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5625, training_loss=2.107128121683167e-05, metrics={'train_runtime': 1679.2264, 'train_samples_per_second': 26.798, 'train_steps_per_second': 3.35, 'total_flos': 5888000332800000.0, 'train_loss': 2.107128121683167e-05, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a1c85-573c-4e65-849e-9854ea1b294f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e0f067-e9f9-4ff2-bb51-19acdaaccc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'This Agreement shall be governed by and construed in accordance with the laws of the State of Delaware, without regard to its conflict of law principles.'\n",
      "\n",
      "Extracted Entities:\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# CORRECTED: Point the path directly to the final checkpoint folder\n",
    "# Replace 'checkpoint-5500' if you see a different number\n",
    "model_path = \"./legal-ner-model/checkpoint-5500\" \n",
    "\n",
    "# The pipeline will now find the config.json and other model files\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=model_path, \n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# The rest of your code remains the same\n",
    "example_text = \"This Agreement shall be governed by and construed in accordance with the laws of the State of Delaware, without regard to its conflict of law principles.\"\n",
    "results = ner_pipeline(example_text)\n",
    "\n",
    "print(f\"Text: '{example_text}'\")\n",
    "print(\"\\nExtracted Entities:\")\n",
    "for entity in results:\n",
    "    print(f\"- Entity: {entity['entity_group']}, Score: {entity['score']:.4f}, Word: '{entity['word']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1045a-c30c-4a2b-b3a8-64ff52553999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Legal AI Project)",
   "language": "python",
   "name": "legal_ai_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
